{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "import json\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler \n",
    "from pyspark.sql.functions import from_utc_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('spark://192.168.100.38:7077')\n",
    "         .appName('MalwareDetection')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.driver.memory\", \"512m\")\n",
    "         .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .config(\"spark.mongodb.input.uri\",\"mongodb://ta:ta@127.0.0.1:27017/MalwareDetection.data?authSource=admin\")\n",
    "         .config(\"spark.mongodb.output.uri\",\"mongodb://ta:ta@127.0.0.1:27017/MalwareDetection.data?authSource=admin\")\n",
    "         .getOrCreate())\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_resample_1400 = pd.read_csv('/work/ta/FinalProject/pa/data/Benign_resample_1400.csv')\n",
    "benign_resample_1400.drop(['Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFormatSchema = spark.read.json(\"schema/schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"netflowmeter\") # topic\n",
    "  .option(\"startingOffsets\", \"latest\") \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = rawData.selectExpr(\"cast (value as string) as json\").select(F.from_json(\"json\",jsonFormatSchema.schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtraction = parsedData.select(F.col('flow_id'), F.col('src_ip'), F.col('src_port'), F.col('dst_ip'), F.col('dst_port'), F.col('protocol'), F.col('timestamp'),F.col(\"extractFeature.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(nested_df):\n",
    "    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n",
    "\n",
    "    flat_df = nested_df.select(flat_cols +\n",
    "                               [F.col(nc+'.'+c).alias(nc+'_'+c)\n",
    "                                for nc in nested_cols\n",
    "                                for c in nested_df.select(nc+'.*').columns])\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat = flatten_df(featureExtraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_columns = ['ActivePacket_max', 'ActivePacket_mean', 'ActivePacket_min', 'ActivePacket_std', 'IdlePacket_max', 'IdlePacket_mean', 'IdlePacket_min','IdlePacket_std',]\n",
    "data_flat = data_flat.drop(*duplicate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler_1 = VectorAssembler(inputCols=df_train.columns, outputCol=\"SS_features\")\n",
    "data_flat = vector_assembler.transform(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data sensor\n",
    "data_flat = scaler.fit(df_train).transform(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_udf(x):\n",
    "    newlist = [x]\n",
    "    z = clf.predict(newlist)\n",
    "    label = int(z[0])\n",
    "    if label == 1:\n",
    "        predict = 'Benign'\n",
    "        return predict\n",
    "    else:\n",
    "        predict = 'Anomaly'\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_udf = F.udf(predict_udf, StringType())\n",
    "data = data_flat.withColumn('Label', label_udf(data_flat['scaledFeatures']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from decimal import Decimal\n",
    "\n",
    "def date_udf(x):\n",
    "    dec = Decimal(x)\n",
    "    c = datetime.fromtimestamp(int(dec)/1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_udf = F.udf(date_udf, StringType())\n",
    "pred_df= data.withColumn('datetime', datetime_udf(data['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom = ['SS_features','scaledFeatures','timestamp']\n",
    "pred_df = pred_df.drop(*kolom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df_1 = data.select(F.col('flow_id'), F.col('src_ip'), F.col('src_port'), F.col('dst_ip'), F.col('dst_port'), F.col('protocol'), F.col('timestamp'), F.col('Label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_mongo_row(df, epoch_id):\n",
    "    df.write.format(\"mongo\").mode(\"append\").option(\"database\",\"MalwareDetection\").option(\"collection\", \"data\").save()\n",
    "    pass\n",
    "\n",
    "query=pred_df.writeStream.foreachBatch(write_mongo_row).start()\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
