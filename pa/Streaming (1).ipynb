{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context mongodb atlas\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('BotnetDetection')\n",
    "         .config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")                     \n",
    "         .config(\"spark.mongodb.input.uri\",\"mongodb+srv://pram:pram123@cluster0.nrqu9.mongodb.net/tes?retryWrites=true&w=majority\")\n",
    "         .config(\"spark.mongodb.output.uri\",\"mongodb+srv://pram:pram123@cluster0.nrqu9.mongodb.net/tes?retryWrites=true&w=majority\")\n",
    "         .getOrCreate())\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context mongodb local\n",
    "# spark = (SparkSession\n",
    "#          .builder\n",
    "#          .master('local')\n",
    "#          .appName('BotnetDetection')\n",
    "#          .config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")                     \n",
    "#          .config(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1:27017/tes1.hasil\")\n",
    "#          .config(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1:27017/tes1.hasil\")\n",
    "#          .getOrCreate())\n",
    "# spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .config(\"spark.mongodb.input.uri\",\"mongodb+srv://pram:pram123@cluster0.nrqu9.mongodb.net/tes?retryWrites=true&w=majority\")\n",
    "# .config(\"spark.mongodb.output.uri\",\"mongodb+srv://pram:pram123@cluster0.nrqu9.mongodb.net/tes?retryWrites=true&w=majority\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('data/SensorTrainDataResampling.csv', header=\"true\", inferSchema =True)\n",
    "#df_train = df_train.select([F.col(column).cast('double') for column in df_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = ['average_packet_size','bwd_IAT_total','bwd_PSH_flags','bwd_URG_flags','bwd_header_length','bwd_packets_per_second','bwd_segment_size_avg','bwd_win_bytes','download_upload_ratio','flow_bytes_per_second','flow_duration','flow_pkts_per_second','fwd_IAT_total','fwd_PSH_flags','fwd_URG_flags','fwd_act_data_pkts','fwd_header_length','fwd_packets_per_second','fwd_seg_size_min','fwd_segment_size_avg','fwd_win_bytes','packet_length_variance','activePacket_max','activePacket_mean','activePacket_min','activePacket_std','bwd_IAT_max','bwd_IAT_mean','bwd_IAT_min','bwd_IAT_std','bwd_bulk_bulk_rate','bwd_bulk_bytes_per_bulk','bwd_bulk_packet_per_bulk','bwd_packet_length_max','bwd_packet_length_mean','bwd_packet_length_min','bwd_packet_length_std','bwd_subflow_subflow_bytes','bwd_subflow_subflow_packets','flagCount_ack','flagCount_cwr','flagCount_ece','flagCount_fin','flagCount_psh','flagCount_rst','flagCount_syn','flagCount_ugr','flow_IAT_max','flow_IAT_mean','flow_IAT_min','flow_IAT_std','fwd_IAT_max','fwd_IAT_mean','fwd_IAT_min','fwd_IAT_std','fwd_bulk_bulk_rate','fwd_bulk_bytes_per_bulk','fwd_bulk_packet_per_bulk','fwd_packet_length_max','fwd_packet_length_mean','fwd_packet_length_min','fwd_packet_length_std','fwd_subflow_subflow_bytes','fwd_subflow_subflow_packets','idlePacket_max','idlePacket_mean','idlePacket_min','idlePacket_std','packet_lenght_max','packet_lenght_mean','packet_lenght_min','packet_lenght_std','totalPacketFeature_backward','totalPacketFeature_forward','totalPacketFeature_length_of_backward','totalPacketFeature_length_of_forward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.select([F.col(column).cast('double') for column in df_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImp = ['fwd_win_bytes','idlePacket_max','fwd_header_length','bwd_packets_per_second','flow_bytes_per_second','bwd_win_bytes','flow_IAT_max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EKSTRAK FITUR UNTUK DATA TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengekstrak fitur data training\n",
    "dfImp = df_train.select(F.col('fwd_win_bytes'), F.col('idlePacket_max'), F.col('fwd_header_length'),F.col('bwd_packets_per_second'), F.col('flow_bytes_per_second'), F.col('bwd_win_bytes'), F.col('flow_IAT_max'),F.col('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfImp = dfImp.select([F.col(column).cast('double') for column in dfImp.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_assembler = VectorAssembler(inputCols=feature, outputCol=\"SS_features\")\n",
    "#df_train = vector_assembler.transform(df_train)\n",
    "#scaler = StandardScaler(inputCol=\"SS_features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=featureImp, outputCol=\"SS_features\")\n",
    "dfImp = vector_assembler.transform(dfImp)\n",
    "scaler = StandardScaler(inputCol=\"SS_features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = scaler.fit(dfImp).transform(dfImp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = scaler.fit(df_train).transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(training_data, test_data) = train.randomSplit([0.8,0.2], seed =2020)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'scaledFeatures', labelCol = 'label', maxDepth =20)\n",
    "dtModel = dt.fit(training_data)\n",
    "dt_predictions = dtModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'accuracy')\n",
    "print('Decision Tree Accuracy:', multi_evaluator.evaluate(dt_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Kafka (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFormatSchema = spark.read.json(\"schema/schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"netflowmeter\") # topic\n",
    "  .option(\"startingOffsets\", \"latest\") \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = rawData.selectExpr(\"cast (value as string) as json\").select(F.from_json(\"json\",jsonFormatSchema.schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengurutkan fitur agar tertata\n",
    "featureExtraction = parsedData.select(F.col('flow_id'), F.col('src_ip'), F.col('src_port'), F.col('dst_ip'), F.col('dst_port'), F.col('protocol'), F.col('timestamp'),F.col(\"extractFeature.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data diflatkan biar ga bercabang --> pada extract feature\n",
    "def flatten_df(nested_df):\n",
    "    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n",
    "\n",
    "    flat_df = nested_df.select(flat_cols +\n",
    "                               [F.col(nc+'.'+c).alias(nc+'_'+c)\n",
    "                                for nc in nested_cols\n",
    "                                for c in nested_df.select(nc+'.*').columns])\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat = flatten_df(featureExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_columns = ['ActivePacket_max', 'ActivePacket_mean', 'ActivePacket_min', 'ActivePacket_std', 'IdlePacket_max', 'IdlePacket_mean', 'IdlePacket_min','IdlePacket_std']\n",
    "data_flat = data_flat.drop(*duplicate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleksi Fitur (perlu coloumn label?)\n",
    "dataSelect =  data_flat.select(F.col('flow_id'), F.col('src_ip'), F.col('src_port'), F.col('dst_ip'), F.col('dst_port'), F.col('protocol'), F.col('timestamp'),F.col('fwd_win_bytes'), F.col('idlePacket_max'), F.col('fwd_header_length'),F.col('bwd_packets_per_second'), F.col('flow_bytes_per_second'), F.col('bwd_win_bytes'), F.col('flow_IAT_max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngevector data berdasarkan df train\n",
    "vector_assembler = VectorAssembler(inputCols=featureImp, outputCol=\"SS_features\")\n",
    "\n",
    "dataSelect = vector_assembler.transform(dataSelect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSelect.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data sensor berdasar df_train\n",
    "#data_flat = scaler.fit(df_train).transform(data_flat)\n",
    "\n",
    "#normalisasi data seleksi fitur\n",
    "dataSelect = scaler.fit(dfImp).transform(dataSelect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = dtModel.transform(dataSelect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from decimal import Decimal\n",
    "\n",
    "def date_udf(x):\n",
    "  dec = Decimal(x)\n",
    "  c = datetime.fromtimestamp(int(dec)/1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_udf = F.udf(date_udf, StringType())\n",
    "pred_df= pred_df.withColumn('datetime', datetime_udf(pred_df['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df_1 = pred_df.select(F.col('flow_id'), F.col('src_ip'), F.col('src_port'), F.col('dst_ip'), F.col('dst_port'), F.col('protocol'), F.col('datetime'), F.col('prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mongo_row(df, epoch_id):\n",
    "    df.write.format(\"mongo\").mode(\"append\").option(\"database\",\"tes\").option(\"collection\",\"hasil\").save()\n",
    "    pass\n",
    "\n",
    "query=pred_df_1.writeStream.foreachBatch(write_mongo_row).start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = pred_df.writeStream.queryName(\"sas\").format(\"memory\").outputMode(\"append\").start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
